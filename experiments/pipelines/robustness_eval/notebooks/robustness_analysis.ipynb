{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5441e-a6a3-425b-98ac-bc73ccc3d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robustness_analysis.ipynb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "results_dir = Path(\"../results/example_run\")\n",
    "metrics_path = results_dir / \"metrics.json\"\n",
    "if not metrics_path.exists():\n",
    "    raise FileNotFoundError(f\"Metrics file not found! Run run.py first: {metrics_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Imports\n",
    "# -----------------------------\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import SVG, display\n",
    "from typing import List, Dict\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Load metrics\n",
    "# -----------------------------\n",
    "results_dir = Path(\"experiments/pipelines/robustness-eval/results/example_run\")\n",
    "metrics_path = results_dir / \"metrics.json\"\n",
    "\n",
    "with open(metrics_path, \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics.head()\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Dataset list\n",
    "# -----------------------------\n",
    "datasets = df_metrics['dataset'].unique()\n",
    "figures_dir = results_dir / \"figures\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Plot heatmaps inline\n",
    "# -----------------------------\n",
    "for ds in datasets:\n",
    "    df_ds = df_metrics[df_metrics['dataset'] == ds]\n",
    "    pivot = df_ds.pivot_table(index=\"missing_rate\", columns=\"noise_level\", values=\"mean_score\")\n",
    "    \n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.imshow(pivot, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Mean AUC\")\n",
    "    plt.xlabel(\"Noise level\")\n",
    "    plt.ylabel(\"Missing rate\")\n",
    "    plt.title(f\"Robustness - {ds}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display saved SVG\n",
    "    svg_file = figures_dir / f\"robustness_{ds}.svg\"\n",
    "    if svg_file.exists():\n",
    "        display(SVG(str(svg_file)))\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Summary table\n",
    "# -----------------------------\n",
    "summary = df_metrics.groupby(\"dataset\").agg({\n",
    "    \"mean_score\": [\"min\", \"mean\", \"max\"]\n",
    "}).round(3)\n",
    "summary\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Threshold analysis\n",
    "# -----------------------------\n",
    "def detect_threshold_crossings(scores: List[float], thresholds: List[float], *, direction=\"higher_is_better\") -> Dict[float, List[int]]:\n",
    "    results = {}\n",
    "    for t in thresholds:\n",
    "        crossings = [i for i, s in enumerate(scores)\n",
    "                     if (s >= t and direction==\"higher_is_better\")\n",
    "                     or (s <= t and direction==\"lower_is_better\")]\n",
    "        results[t] = crossings\n",
    "    return results\n",
    "\n",
    "# Define target threshold for AUC\n",
    "target_threshold = 0.9\n",
    "threshold_summary = {}\n",
    "\n",
    "for ds in datasets:\n",
    "    df_ds = df_metrics[df_metrics['dataset'] == ds].sort_values(by=[\"missing_rate\", \"noise_level\"])\n",
    "    scores = df_ds['mean_score'].tolist()\n",
    "    crossings = detect_threshold_crossings(scores, thresholds=[target_threshold], direction=\"lower_is_better\")\n",
    "    threshold_summary[ds] = crossings\n",
    "\n",
    "threshold_summary\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Visualize threshold crossings\n",
    "# -----------------------------\n",
    "for ds in datasets:\n",
    "    df_ds = df_metrics[df_metrics['dataset'] == ds].pivot_table(\n",
    "        index=\"missing_rate\", columns=\"noise_level\", values=\"mean_score\"\n",
    "    )\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.imshow(df_ds, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Mean AUC\")\n",
    "    plt.xlabel(\"Noise level\")\n",
    "    plt.ylabel(\"Missing rate\")\n",
    "    plt.title(f\"{ds} robustness with threshold {target_threshold}\")\n",
    "    \n",
    "    # Overlay threshold crossings\n",
    "    crossings = threshold_summary[ds][target_threshold]\n",
    "    for idx in crossings:\n",
    "        row = idx // len(df_ds.columns)\n",
    "        col = idx % len(df_ds.columns)\n",
    "        plt.scatter(col, row, color=\"red\", marker=\"x\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 8️⃣ Notes / Applied ML signal\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "- Evaluated robustness across structured CS/Math datasets (Iris, Digits, Wine)\n",
    "- Noise levels: [0.0, 0.05, 0.1], Missingness rates: [0.0, 0.1, 0.2]\n",
    "- Multi-class AUC computed with multi_class='ovr'\n",
    "- Pipeline: SimpleImputer + RandomForestClassifier\n",
    "- Figures: heatmaps of mean AUC; red X marks cells where mean AUC < threshold (0.9)\n",
    "- Metrics saved to metrics.json, figures saved as SVG\n",
    "- Signals applied ML judgment: sensitivity to noise/missingness, robustness evaluation, reproducibility\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
